{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i followed the following tutorial\n",
    "#https://towardsdatascience.com/latent-variables-expectation-maximization-algorithm-fb15c4e0f32c\n",
    "#https://github.com/suvoooo/Machine_Learning/blob/master/ExMax_ALgo/LVM.ipynb\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4, 1, 4],\n",
       "       [2, 2, 3, 3]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([[1, 2],[4, 2], [1,3], [4,3]])\n",
    "data2 = np.array([[1,4,1,4], [2,2,3,3]])\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### E step for GMM \n",
    "### Use multivariate normal for Gaussian distribution \n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def E_step(data, pi, mu, sigma):\n",
    "  N = data.shape[0] # number of data-points\n",
    "  K = pi.shape[0] # number of clusters, following notation used before in description\n",
    "    \n",
    "  d = mu.shape[1] # dimension of each data point, think of these as attributes\n",
    "  \n",
    "  gamma = np.zeros((N, K)) # this is basically responsibility which should be equal to posterior. \n",
    "\n",
    "  for nk in range(K):\n",
    "    gamma[:, nk] = pi[nk] * multivariate_normal.pdf(data, mean=mu[nk], cov=sigma[nk]) \n",
    "    # calculate responsibility for each cluster\n",
    "  gamma = gamma/np.sum(gamma, axis=1, keepdims=True) \n",
    "  # use the sum over all the clusters, thus axis=1. Denominator term. \n",
    "  # print (\"gamma shape: \", gamma.shape)\n",
    "  return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def M_step(data, gamma):\n",
    "  N, D = data.shape \n",
    "  K = gamma.shape[1] # use the posterior shape calculated in E-step to determine the no. of clusters  \n",
    "  pi = np.zeros(K)\n",
    "  mu = np.zeros((K, D))\n",
    "  sigma = np.zeros((K, D, D)) \n",
    "\n",
    "  for ik in range(K):\n",
    "    n_k = gamma[:, ik].sum() # we use the definintion of N_k \n",
    "    pi[ik] = n_k/N # definition of the weights\n",
    "    elements = np.reshape(gamma[:, ik], (gamma.shape[0], 1)) \n",
    "    # get each columns and reshape it (K, 1) form so that later broadcasting is possible. \n",
    "    mu[ik,:] = (np.multiply( elements,  data)).sum(axis=0) / n_k  \n",
    "    sigma_sum = 0.\n",
    "    for i in range(N):\n",
    "      var = data[i] - mu[ik]\n",
    "      sigma_sum = sigma_sum + gamma[i, ik] * np.outer(var, var)# outer product creates the covariance matrix\n",
    "    sigma[ik,:] = sigma_sum/n_k    \n",
    "  return pi, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def elbo(data, gamma, pi, mu, sigma):\n",
    "  N = data.shape[0] # no. of data-points\n",
    "  K = gamma.shape[1] # no. of clusters\n",
    "  d = data.shape[1] # dim. of each object\n",
    "\n",
    "  loss = 0.\n",
    "  for i in range(N):\n",
    "      x = data[i]\n",
    "      for k in range(K):\n",
    "        pos_dist = gamma[i, k] ## p(z_i=k|x) = gamma_ik\n",
    "        log_lik = np.log(multivariate_normal.pdf(x, mean=mu[k, :], cov=sigma[k, :, :]) + 1e-20) # log p(x|z)\n",
    "        log_q = np.log(gamma[i, k] + 1e-20) # log q(z) = log p(z_i=k|x)\n",
    "        log_pz = np.log(pi[k] + 1e-20)  # log p(z_k =1) =\\pi _k\n",
    "        loss = (loss + np.multiply(pos_dist, log_pz) + np.multiply(pos_dist, log_lik) +  \n",
    "        np.multiply(pos_dist, -log_q) )\n",
    "  #print (\"check loss: \", loss)\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data, K, tolerance=1e-3, max_iter=50, restart=10):\n",
    "  N, d = data.shape\n",
    "  elbo_best = -np.inf # loss set to the lowest value \n",
    "  pi_best = None\n",
    "  mu_best = None\n",
    "  sigma_best = None\n",
    "  gamma_f = None\n",
    "  for _ in range(restart):\n",
    "    pi = np.ones(K) / K # if 3 clusters then an array of [.33, .33, .33] # the sum of pi's should be one \n",
    "    # that's why normalized  \n",
    "    #you can change this to match the slides\n",
    "    mu = np.random.rand(K, d) # no condition on \n",
    "    #mu = np.array([2.5, 2.5])\n",
    "    sigma = np.tile(np.eye(d), (K, 1, 1)) # to initialize sigma we first start with ones only at the diagonals\n",
    "    #sigma = np.tile([1.7321, 0.57735])\n",
    "    # the sigmas are postive semi-definite and symmetric  \n",
    "    last_iter_loss = None\n",
    "    all_losses = []\n",
    "    try:\n",
    "\n",
    "      for i in range(max_iter):\n",
    "        gamma = E_step(data, pi, mu, sigma)\n",
    "        pi, mu, sigma = M_step(data, gamma)\n",
    "        loss = elbo(data, gamma, pi, mu, sigma)\n",
    "        if loss > elbo_best:\n",
    "          elbo_best = loss\n",
    "          pi_best = pi \n",
    "          mu_best = mu\n",
    "          sigma_best = sigma\n",
    "          gamma_f = gamma\n",
    "        if last_iter_loss and abs((loss-last_iter_loss)/last_iter_loss) < tolerance: # insignificant improvement\n",
    "          break \n",
    "        last_iter_loss = loss\n",
    "        all_losses.append(loss)\n",
    "    except np.linalg.LinAlgError: # avoid the delta function situation \n",
    "      pass \n",
    "\n",
    "  return elbo_best, pi_best, mu_best, sigma_best, all_losses, gamma_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss, pi_best, mu_best, sigma_best, ls_lst, final_posterior = train_loop(data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loss, best_loss:  -10.196246223377141 , -0.8434791419504601\n",
      "best pi [0.27238556 0.72761444]\n",
      "best mu:  [[1.00000002 2.43793519]\n",
      " [3.06153139 2.52323423]]\n",
      "best sigma:  [[[ 5.41145113e-08 -1.27739223e-09]\n",
      "  [-1.27739223e-09  2.46147959e-01]]\n",
      "\n",
      " [[ 1.93468250e+00 -4.78980852e-02]\n",
      "  [-4.78980852e-02  2.49460171e-01]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (\"starting loss, best_loss: \", ls_lst[0], ',',  best_loss)\n",
    "print (\"best pi\", pi_best) \n",
    "print (\"best mu: \", mu_best)\n",
    "\n",
    "print (\"best sigma: \", sigma_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
